<p><a href="https://rssaketh.github.io/project_pages/obj_disc_new.html#workshop_challenge" target="_blank">Object discovery </a>is the task of automatically identifying and
    grouping semantically coherent objects without human intervention. Discovery algorithms typically address several challenges like novelty detection, open world recognition
    and clustering, capabilities which are essential for systems deployed in-the-wild. This year, to facilitate discussions among researchers who have different backgrounds,
    we host a teaser challenge which studies a systemâ€™s capabilities to discover and group novel object categories in a large unlabeled dataset.
</p>.

<h4>Important Dates</h4>
<ul>
<li>Apr 29, 2022, Challenge announced</li>
<li>May 23, 2022, EvalAI open for submissions</li>
<li>June 10, 2022, Challenge will be closed</li>
<li>June 15, 2022, Results will be releases and participants will be invited for presentation</li>
<li>June 20, 2022, Workshop@CVPR,22</li>
</ul>

<h4>Protocol of Algorithm Design</h4>
<p> Proposed algorithms for this challenge will operate on widely used object detection datasets, i.e. <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/" target="_blank">PASCAL VOC 2007 </a>
    and <a href="https://cocodataset.org/" target="_blank"> COCO 2014 </a>, and report results on two splits. </p>
<p>
Object discovery systems should be very careful about the assumptions of prior knowledge. To be precise, initializations used by algorithms should never be exposed/trained on categories
    they wish to discover. This curtails the pre-training of any network on the <a href="https://www.image-net.org/" target="_blank">ImageNet</a> dataset using labeled data. </p>

<p>
While we do not discourage teams from using weights of networks trained on ImageNet using labels, only weights trained using self-supervision will be considered for ranking on the
leaderboard. While there is no restriction on the usage of models, teams should compare the model performance to a standard  ResNet-50 network trained using
    <a href="https://github.com/facebookresearch/dino" target="_blank">DINO algorithm. </a>
Teams should describe in detail, without fail, each component used and the amount of improvement they offer.

</p>

<h4>Task and Evaluation Metrics</h4>
<p>
The task of the challenge is to discover novel objects in a large corpus of unlabeled images using knowledge about known objects. Specifically, given a labeled dataset with K known
objects and a large unlabeled dataset consisting of U (not known a-priori) unknown objects,  algorithms should output a set of M (not known a-priori) clusters, each of which contains
regions belonging to an object class. Additionally, an object detector should be trained for each cluster whose performance is evaluated on a held-out set to assess the real world
applicability of the object discovery system.</p>
<p>
The primary evaluation metric is the Area under the curve of Purity coverage plots and PASCAL VOC style mean Average Precision at an IoU threshold of 0.50. Additionally we also
    report the number of clusters, number of objects discovered, Correct Localization. </p>


<h4>Dataset</h4>
All systems submitted to the challenge are allowed to use three datasets, namely ImageNet 2012, PASCAL VOC 2007 and COCO 2014 datasets. See respective websites for the dataset formats.

<p>
<strong>Labeled dataset:</strong> The object detection dataset, PASCAL VOC 2007 split is considered as the labeled dataset for this challenge. Teams can use the region level labels to
    train object detection models for their submissions. We assume the 20 categories of PASCAL-VOC as the known categories.</p>
<p><strong>Discovery Set:</strong> The COCO 2014 train set,  without any labels, is used as the discovery dataset. The remaining categories, not common with PASCAL-VOC, are considered
the novel categories. Teams are encouraged not to assume the number of novel categories to be known a-priori.</p>
<p><strong>Pre-training dataset:</strong> To train object detection datasets, ImageNet pre-training is a standard practice. Teams can leverage self-supervised or supervised learning to
obtain weights for initialization. However, only the results using self-supervised learning will be considered for ranking.</p>
<p><strong>Evaluation set:</strong> All systems will be evaluated on the discovery performance and object detection performance. For object discovery, results are reported on the COCO
2014 train set. For object detection on the 20 known classes and the newly discovered objects, results will be reported on the COCO minival set.</p>



<h4>Contact Information</h4>
<p>
Organizers: Anubhav (UMD), Pulkit Kumar (UMD), Saketh Rambhatla (UMD) , Abhinav Shrivastava (UMD)
</p>

<p>
For questions regarding technical details, feel free to submit issues on our <a href="https://github.com/learn2phoenix/cvpr22_vplow_ow">github repo</a>. If you have high-level questions, please contact Anubhav (anubhav@umd.edu)
    and/or Pulkit (pulkit@umd.edu)
</p>

<h4>Acknowledgement</h4>
This challenge is supported by the DARPA SAIL-ON project. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reflect the views of funding agents and sponsors.

